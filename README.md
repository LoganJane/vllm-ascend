<!-- markdownlint-disable MD001 MD041 -->
<h1 align="center">
Kimi-K2.5 æ¨¡å‹åŸºäºvLLM-Ascend æ¨ç†æŒ‡å¯¼
</h1>

## ä¸€ã€å‡†å¤‡è¿è¡Œç¯å¢ƒ

**è¡¨ 1**  ç‰ˆæœ¬é…å¥—è¡¨

| é…å¥—  | ç‰ˆæœ¬ | ç¯å¢ƒå‡†å¤‡æŒ‡å¯¼ |
| ----- | ----- |-----|
| Python | 3.11.10 | - |
| torch | 2.9.0 | - |
| transformers | 4.57.6 | - |

### 1.1 è·å–vllm-ascendé•œåƒ

- [é•œåƒé“¾æ¥](https://quay.io/repository/ascend/vllm-ascend?tab=tags)ï¼šhttps://quay.io/repository/ascend/vllm-ascend?tab=tags
- ä¸‹è½½é•œåƒå‘½ä»¤
```shell
docker pull quay.io/ascend/vllm-ascend:v0.14.0rc1
```

### 1.2 ç‰¹æ€§åˆ†æ”¯

- vllmï¼šhttps://github.com/LoganJane/vllm/tree/main
- vllm-ascendï¼šhttps://github.com/LoganJane/vllm-ascend/tree/main

### 1.3 ç¤¾åŒºé€‚é…PR

- vllmï¼šhttps://github.com/vllm-project/vllm/pull/34501
- vllm-ascendï¼šhttps://github.com/vllm-project/vllm-ascend/pull/6755

### 1.4 å®‰è£… vllm & vllm-ascend

```shell
# å¸è½½é•œåƒä¸­vllm/vllm-ascend
pip uninstall -y vllm vllm_ascend
```

```shell
# å®‰è£…vllm
git clone https://github.com/LoganJane/vllm.git
cd vllm
VLLM_TARGET_DEVICE=empty pip install -v -e .
```

```shell
# è®¾ç½®ç¯å¢ƒå˜é‡
source /usr/local/Ascend/ascend-toolkit/set_env.sh
source /usr/local/Ascend/nnal/atb/set_env.sh
# å®‰è£…vllm-ascend
git clone https://github.com/LoganJane/vllm-ascend.git
cd vllm-ascend
pip install -v -e .
```

## äºŒã€ä¸‹è½½æƒé‡

### 2.1 vLLM-Ascend w4a8æƒé‡

- ModelScope

|  æ¨¡å‹ | é“¾æ¥  |
| ------------ | ------------ |
| Eco-Tech/Kimi-K2.5-W4A8  |  [ModelScope](https://modelscope.cn/models/Eco-Tech/Kimi-K2.5-W4A8/files) |

### 2.2 Kimi-K2.5 å®˜æ–¹æƒé‡

- Huggingface

|  æ¨¡å‹ | é“¾æ¥  |
| ------------ | ------------ |
| moonshotai/Kimi-K2.5  |  [ğŸ¤—huggingface](https://huggingface.co/moonshotai/Kimi-K2.5/tree/main) |

- ModelScope

|  æ¨¡å‹ | é“¾æ¥  |
| ------------ | ------------ |
| moonshotai/Kimi-K2.5  |  [ModelScope](https://modelscope.cn/models/moonshotai/Kimi-K2.5/files) |

## ä¸‰ã€Atlas 800I A3 å•æœºæ··éƒ¨éƒ¨ç½²

### 3.1 w4a8é‡åŒ–æƒé‡å¯åŠ¨å‘½ä»¤

```shell
#!/bin/sh
export OMP_PROC_BIND=false
export OMP_NUM_THREADS=1
export PYTORCH_NPU_ALLOC_CONF="expandable_segments:True"
export VLLM_USE_V1=1
export TASK_QUEUE_ENABLE=1
export VLLM_TORCH_PROFILER_WITH_STACK=0

export LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libjemalloc.so.2:$LD_PRELOAD
echo performance | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
sysctl -w vm.swappiness=0
sysctl -w kernel.numa_balancing=0
sysctl -w kernel.sched_migration_cost_ns=50000

export VLLM_ASCEND_BALANCE_SCHEDULING=1
export HCCL_BUFFSIZE=1536
export VLLM_ASCEND_ENABLE_FUSED_MC2=1

vllm serve /weights/Kimi-K2.5-W4A8 \
    --served-model-name kimi \
    --tool-call-parser kimi_k2 \
    --reasoning-parser kimi_k2 \
    --quantization ascend \
    --trust-remote-code \
    --tensor-parallel-size 8 \
    --data-parallel-size 2 \
    --enable-expert-parallel \
    --port 8008 \
    --max-num-seqs 256 \
    --max-model-len 32768 \
    --max-num-batched-tokens 12288 \
    --no-enable-prefix-caching \
    --gpu-memory-utilization 0.9 \
    --allowed-local-media-path / \
    --seed 42 \
    --async-scheduling \
    --mm-processor-cache-type shm \
    --mm-encoder-tp-mode data \
    --compilation-config '{"cudagraph_capture_sizes":[256,192,160,128,96,64,32,16,8,4,2,1], "cudagraph_mode":"FULL_DECODE_ONLY"}' \
    --additional-config '{"ascend_scheduler_config":{"enabled":false},"torchair_graph_config":{"enabled":false}}'
```

### 3.2 å®˜æ–¹åŸå§‹æƒé‡å¯åŠ¨å‘½ä»¤

```shell
#!/bin/sh
export OMP_PROC_BIND=false
export OMP_NUM_THREADS=1
export PYTORCH_NPU_ALLOC_CONF="expandable_segments:True"
export VLLM_USE_V1=1
export TASK_QUEUE_ENABLE=1
export VLLM_TORCH_PROFILER_WITH_STACK=0

export LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libjemalloc.so.2:$LD_PRELOAD
echo performance | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
sysctl -w vm.swappiness=0
sysctl -w kernel.numa_balancing=0
sysctl -w kernel.sched_migration_cost_ns=50000

export VLLM_ASCEND_BALANCE_SCHEDULING=1

vllm serve /weights/Kimi-K2.5 \
    --served-model-name kimi \
    --tool-call-parser kimi_k2 \
    --reasoning-parser kimi_k2 \
    --trust-remote-code \
    --tensor-parallel-size 8 \
    --data-parallel-size 2 \
    --enable-expert-parallel \
    --port 8008 \
    --max-num-seqs 256 \
    --max-model-len 32768 \
    --max-num-batched-tokens 12288 \
    --no-enable-prefix-caching \
    --gpu-memory-utilization 0.9 \
    --allowed-local-media-path / \
    --seed 42 \
    --async-scheduling \
    --mm-processor-cache-type shm \
    --mm-encoder-tp-mode data \
    --compilation-config '{"cudagraph_capture_sizes":[256,192,160,128,96,64,32,16,8,4,2,1], "cudagraph_mode":"FULL_DECODE_ONLY"}' \
    --additional-config '{"ascend_scheduler_config":{"enabled":false},"torchair_graph_config":{"enabled":false}}'
```

### 3.3 å‘é€å¤šæ¨¡æ€è¯·æ±‚æ‰§è¡Œæ¨ç†

```shell
curl http://localhost:8008/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "kimi",
        "messages": [{
            "role": "user",
            "content": [
            {
                "type": "image_url",
                "image_url": {
                    "url": "file:///datasets/test.jpg",
                    "detail": "high"
                }
            },
            {
                "type": "text",
                "text": "è¯·æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹ã€‚"
            }]
        }],
        "max_tokens": 1024
    }'
```

## å››ã€Atlas 800I A3 åŒæœºPDåˆ†ç¦»éƒ¨ç½²

### 4.1 PD proxy

```shell
python vllm-ascend/examples/disaggregated_prefill_v1/load_balance_proxy_server_example.py \
    --host 0.0.0.0 \
    --port 8008 \
    --prefiller-hosts 102.34.56.78 \
    --prefiller-port 1025 \
    --decoder-hosts 102.34.56.79 \
    --decoder-ports 1026
```

### 4.2 Prefill node 0

```shell
nic_name="enp48s3u1u1"
local_ip="102.34.56.78"

export HCCL_IF_IP=$local_ip
export GLOO_SOCKET_IFNAME=$nic_name
export TP_SOCKET_IFNAME=$nic_name
export HCCL_SOCKET_IFNAME=$nic_name

export OMP_PROC_BIND=false
export OMP_NUM_THREADS=10
export PYTORCH_NPU_ALLOC_CONF="expandable_segments:True"
export VLLM_USE_V1=1
export TASK_QUEUE_ENABLE=1
export VLLM_TORCH_PROFILER_WITH_STACK=0
export VLLM_RPC_TIMEOUT=3600000
export VLLM_EXECUTE_MODEL_TIMEOUT_SECONDS=3600000

export VLLM_WORKER_MULTIPROC_METHOD="fork"
export ASCEND_BUFFER_POOL=4:8
export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/mooncake:$LD_LIBRARY_PATH
export LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libjemalloc.so.2:$LD_PRELOAD
echo performance | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
sysctl -w vm.swappiness=0
sysctl -w kernel.numa_balancing=0
sysctl -w kernel.sched_migration_cost_ns=50000

export VLLM_ASCEND_ENABLE_MLAPO=1
export HCCL_BUFFSIZE=800
export VLLM_ASCEND_ENABLE_FUSED_MC2=1

vllm serve /weights/Kimi-K2.5-W4A8 \
    --host 0.0.0.0 \
    --port 1025 \
    --quantization ascend \
    --served-model-name kimi \
    --tool-call-parser kimi_k2 \
    --reasoning-parser kimi_k2 \
    --trust-remote-code \
    --tensor-parallel-size 8 \
    --data-parallel-size 2 \
    --enable-expert-parallel \
    --max-num-seqs 8 \
    --max-model-len 16384 \
    --max-num-batched-tokens 8192 \
    --no-enable-prefix-caching \
    --gpu-memory-utilization 0.9 \
    --allowed-local-media-path / \
    --seed 42 \
    --async-scheduling \
    --mm-processor-cache-type shm \
    --mm-encoder-tp-mode data \
    --additional-config '{"ascend_scheduler_config":{"enabled":false},"torchair_graph_config":{"enabled":false}}' \
    --kv-transfer-config \
    '{"kv_connector": "MooncakeConnectorV1",
    "kv_role": "kv_producer",
    "kv_port": "30100",
    "engine_id": "0",
    "kv_connector_module_path": "vllm_ascend.distributed.mooncake_connector",
    "kv_connector_extra_config": {
                "prefill": {
                        "dp_size": 2,
                        "tp_size": 8
                },
                "decode": {
                        "dp_size": 4,
                        "tp_size": 4
                }
        }
    }'
```

### 4.3 Decode node 0

```shell
nic_name="enp48s3u1u1"
local_ip="102.34.56.79"

export HCCL_IF_IP=$local_ip
export GLOO_SOCKET_IFNAME=$nic_name
export TP_SOCKET_IFNAME=$nic_name
export HCCL_SOCKET_IFNAME=$nic_name

export OMP_PROC_BIND=false
export OMP_NUM_THREADS=10
export PYTORCH_NPU_ALLOC_CONF="expandable_segments:True"
export VLLM_USE_V1=1
export TASK_QUEUE_ENABLE=1
export VLLM_TORCH_PROFILER_WITH_STACK=0
export VLLM_RPC_TIMEOUT=3600000
export VLLM_EXECUTE_MODEL_TIMEOUT_SECONDS=3600000

export VLLM_WORKER_MULTIPROC_METHOD="fork"
export ASCEND_BUFFER_POOL=4:8
export LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/mooncake:$LD_LIBRARY_PATH
export LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libjemalloc.so.2:$LD_PRELOAD
echo performance | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor
sysctl -w vm.swappiness=0
sysctl -w kernel.numa_balancing=0
sysctl -w kernel.sched_migration_cost_ns=50000

export VLLM_ASCEND_ENABLE_MLAPO=1
export HCCL_BUFFSIZE=1024
export VLLM_ASCEND_ENABLE_FUSED_MC2=1

vllm serve /weights/Kimi-K2.5-W4A8 \
    --host 0.0.0.0 \
    --port 1026 \
    --quantization ascend \
    --served-model-name kimi \
    --tool-call-parser kimi_k2 \
    --reasoning-parser kimi_k2 \
    --trust-remote-code \
    --tensor-parallel-size 4 \
    --data-parallel-size 4 \
    --enable-expert-parallel \
    --max-num-seqs 32 \
    --max-model-len 32768 \
    --max-num-batched-tokens 128 \
    --no-enable-prefix-caching \
    --gpu-memory-utilization 0.9 \
    --allowed-local-media-path / \
    --seed 42 \
    --async-scheduling \
    --mm-processor-cache-type shm \
    --mm-encoder-tp-mode data \
    --compilation-config '{"cudagraph_capture_sizes":[1,2,4,8,16,32,64,96,128], "cudagraph_mode":"FULL_DECODE_ONLY"}' \
    --additional-config '{"ascend_scheduler_config":{"enabled":false},"torchair_graph_config":{"enabled":false}}' \
    --kv-transfer-config \
    '{"kv_connector": "MooncakeConnectorV1",
    "kv_role": "kv_consumer",
    "kv_port": "30200",
    "engine_id": "1",
    "kv_connector_module_path": "vllm_ascend.distributed.mooncake_connector",
    "kv_connector_extra_config": {
                "prefill": {
                        "dp_size": 2,
                        "tp_size": 8
                },
                "decode": {
                        "dp_size": 4,
                        "tp_size": 4
                }
        }
    }'
```

### 4.4 å‘é€å¤šæ¨¡æ€è¯·æ±‚æ‰§è¡Œæ¨ç†

```shell
curl http://localhost:8008/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "kimi",
        "messages": [{
            "role": "user",
            "content": [
            {
                "type": "image_url",
                "image_url": {
                    "url": "file:///datasets/test.jpg",
                    "detail": "high"
                }
            },
            {
                "type": "text",
                "text": "è¯·æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹ã€‚"
            }]
        }],
        "max_tokens": 1024
    }'
```
